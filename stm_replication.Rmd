---
title: "Whose ideas are worth spreading? <br>The representation of women and ethnic groups in TED talks - A Small Replication using R"
author: "Camille Landesvatter"
date: "June 1, 2021"
output:
  html_document: 
    highlight: tango
    toc: true
    toc_depth: 3
    code_folding: "show"
---

## Introduction

The following demonstrates an analysis with data of [TED talk transcripts](https://www.ted.com/talks).
Originally, this research and the collected data stems from research of Carsten Schwemmer & Sebastian Jungkunz and their paper ["Whose ideas are worth spreading? The representation of women and ethnic groups in TED talks"](https://www.tandfonline.com/doi/full/10.1080/2474736X.2019.1646102) (Political Research Exchange **2019** (1), 1-23).


If you are interested in the following analyses, definitely go and read their [original paper](https://www.tandfonline.com/doi/full/10.1080/2474736X.2019.1646102). In short, they answered the question of how women and different ethnic groups are representated in TED talks. The data they gathered contains transcripts of over 2333 TED talks. Further, via a facial recognition API they were able to extract key characteristics of speakers, such as gender and ethnicity. Finally, by using the Youtube Data API they also extracted YT metadata, such as likes, comments and views.


With this immense data they were able to perform multiple analyses. In the following I will solely focus and mimic their part on topic models. Here, they fitted a [structural topic model](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf) to **a)** examine how many and which topics are discussed in TED talks and **b)** how gender and ethnicity influence the topic prevalence (*e.g., are women more likely to talk about technical topics than men?*).

Extensive replication files with R code, plots and data are made available via the [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/EUDWP3).

**Other resources**:

Apart from benefiting from their excellent replication files which everyone can easily access, there are other resources, who have used the TED data to demonstrate topic models. See e.g., this [script](https://bookdown.org/joone/ComputationalMethods/topicmodeling.html#topic-modeling-mit-stm) for a complete workflow in german language.

For further ideas, especially with regards to visualization and pre-processing of textual data have a look at [Julia Silges](https://juliasilge.com/) work. Together with [David Robinson](http://varianceexplained.org/) she also published excellent guidelines on the [tidytext R package](https://www.tidytextmining.com/). [Here](https://juliasilge.com/blog/sherlock-holmes-stm/) and [here](https://juliasilge.com/blog/evaluating-stm/) are two blog posts on the specific task of Structural Topic Modely by Julia Silge:

## Setup

```{r, include = FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/camil/Desktop/ted-talks-STM') #set working directory

set.seed(1337)
```

### Load packages

We start by loading all necessary R packages. The below code also ensures that packages are installed if needed.

```{r message=FALSE, warning=FALSE}
if(!require("tidyverse")) {install.packages("tidyverse"); library("tidyverse")}

if(!require("quanteda")) {install.packages("quanteda"); library("quanteda")}
if(!require("tidytext")) {install.packages("tidytex"); library("tidytext")}
if(!require("stm")) {install.packages("stm"); library("stm")}
if(!require("stminsights")) {install.packages("stminsights"); library("stminsights")}

if(!require("lubridate")) {install.packages("lubridate"); library("lubridate")}
if(!require("ggplot2")) {install.packages("ggplot2"); library("ggplot2")}
if(!require("cowplot")) {install.packages("cowplot"); library("cowplot")}

if(!require("scales")) {install.packages("scales"); library("scales")}
if(!require("ggthemes")) {install.packages("ggthemes"); library("ggthemes")}

theme_set(theme_light()) #set global theme
options(scipen=999) #force R not to use exponential notation (e.g. e+10)
```

### Data Import

Data is made available on the [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/EUDWP3). Besides basic operations (e.g, removing talks without human speakers), I also take a subset which contains the following information:

- ``id``: numerical identifier of each TED talk
- ``title``: title of the TED talk
- ``date_num``: year of the TED talk
- ``text``: transcript of the TED talk
- ``dummy_female``: a dummy variable for speaker gender (female vs male)
- ``dummy_female``: a dummy variable for speaker ethnicity (white vs non-white)

```{r message=FALSE, warning=FALSE}
df <- read_tsv('C:/Users/camil/Desktop/CSS Session/ted_main_dataset.tsv')

df$date_num <- factor(df$date) %>% as.numeric() # turn date variable from character to numeric

df <- df %>% 
  filter(!is.na(speaker_image_nr_faces)) %>% # remove talks without human speakers
  mutate(id = 1:n()) %>% # add numeric ID for each talk
  select(id, date_num, title, text, dummy_female, dummy_white) # select relevant variables

head(df, 5)
```

# Substantive Representation within TED Talks

In the following, I will calculate a topic model, namely a structural topic model, which the authors fitted in order to **a)** examine how many and which topics are discussed in TED talks and **b)** how gender and ethnicity of a speaker influences the topic prevalende (*e.g., are women more likely to talk about technical topics than men?*).

## Data Pre-processing

Before we can fit such a structural topic model (and any topic model or text-analytical analyses at all) we have to invest some time in data pre-processing. 

### Step 1: Create a Corpus

To create a corpus of TED Talks and to apply neccessary cleaning functions I here use the [{quanteda}-package](https://cran.r-project.org/web/packages/quanteda/index.html). There are very good alternatives, for example [{tidytext}](https://cran.r-project.org/web/packages/tidytext/index.html) or [{tm}](https://cran.r-project.org/web/packages/tm/index.html).

```{r message=FALSE, warning=FALSE}
ted_corpus <- corpus(df$text)
ted_corpus[1:5]
```

### Step 2: Clean Corpus and cast data into a DFM/DTM

Now we engage in multiple data manipulation steps, for instance we remove all punctuation (remove_punct) and convert all characters to lower case (tolower). At the same time, we convert our corpus object into a document-feature matrix (DFM).

```{r message=FALSE, warning=FALSE}
# with quanteda (combines cleaning and casting in one step)
ted_dfm <- dfm(ted_corpus,
            stem = TRUE,
            tolower = TRUE,
            remove_punct = TRUE,
            remove_numbers =TRUE,
            verbose = TRUE,
            remove = stopwords('english'))
ted_dfm #check that stemming etc worked
dim(ted_dfm)
```

The document-feature matrix we just constructed contains 2333 documents with 44704 features (i.e., terms). Because of the extremely large number of features we can engage in feature trimming (do not trim for already small corpora!) to reduce the size of the DFM.
Ideally, we exclude features that occur way too often or too few times to be meaningful interpreted. When you decide to trim, be careful and compare different parameters, since this step might strongly influence the results of your topic model.
Here, we closely follow the original paper and exclude all terms that appear in more than half or in less than 1% of all talks.

```{r message=FALSE, warning=FALSE}
ted_dfm <- dfm_trim(ted_dfm, max_docfreq = 0.50,min_docfreq = 0.01,
                    docfreq_type = 'prop')

dim(ted_dfm) # 2333 documents with 4805 features (previous: 44727)
```

## Analysis
### (Structural) Topic Model

Our text data is now prepared. Before we can fit the actual STM we have to consider that each topic model implementation in R (e.g., LDA, biterm topic model, structural topic models) needs a different input format. The stm-package which I will use in the following needs a different format than the quanteda object we just designed $\rightarrow$ use the {quanteda}-convert function to construct such an object.

```{r message=FALSE, warning=FALSE}
out <- convert(ted_dfm, to = "stm", docvars=df)

str(out, max.level = 1)
```

Now, using this object containing all 2333 documents and their respective vocabulary we can perform our first topic model.

The following chunk will take several minutes to evaluate (for me 5-10 minutes). If you want to run it yourself, set eval to TRUE in the next code chunk (for now it is set to FALSE).

```{r, eval = FALSE}
# running model with 20 topics with covariates
stm_20 <- stm(
  documents=out$documents,
  vocab=out$vocab,
  data = out$meta,
  prevalence =  ~ s(date_num, degree = 2) + dummy_female * dummy_white,
  init.type = 'Spectral', #default
  K = 20,
  verbose = FALSE
)
```

```{r, eval = FALSE, include=FALSE}
save.image('C:/Users/camil/Desktop/CSS Session/ted_first_model.RData')
```

```{r message=FALSE, warning=FALSE, include=FALSE}
load("C:/Users/camil/Desktop/CSS Session/ted_first_model.RData")
```

We can also plot the results of our topic model.

```{r message=FALSE, warning=FALSE}
# plot topics with most frequent words within topics
plot(stm_20)

# By using summary() you receive top words for all 20 topics
summary(stm_20)
```

The huge advantage of STM above other topic model implementations in R is that we can include covariates to examine influences of those on topic prevelance. Such research questions are extremely interesting for description as well as explanation within social scientific research questions (*e.g., are women more likely to talk about technical topics than men?*).

Here I only use the summary() function to display regression coefficients for each topic. This however results in 20 rather long tables, which we shouldn't report in this way.

The original paper contains some [great plots](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/EUDWP3/MCZREF&version=2.0) that visualizes findings from the regression analysis.

```{r message=FALSE, warning=FALSE}
# fit regression of topic prevalence on gender and ethnicity
effect_stm_20 <- estimateEffect(1:20 ~ s(date_num, degree = 2) +
                                  dummy_female * dummy_white,
                 stm_20,
                 metadata = out$meta)

#summary() gives you regression coefficients for all 20 topics
#summary(effect_stm_20)
```

## The optimal number of k 

This far, we simply decided to set k (= the number of topics) to 20, i.e. we expect our selection of TED talks to cover 20 topics. This was pure guessing. There are many different ways of how to specify k (informed guessing via theory and previous research, [rule of thumb](https://cran.r-project.org/web/packages/stm/stm.pdf), manual labeling of a subset, **statistical measures**).

In the following, we will focus on statistical measures, which is a comparatively fast and reasonable way of validating your **k**. To do so:

1. perform multiple topic models with multiple specifications of k
2. compare those on basis of statistical measures
3. choose one specification of k
4. finally visualizise and manual interpretate your topic model of choice

```{r, eval = FALSE}
# specify models with different k
stm_20_30_40_50 <- tibble(K = c(20, 30, 40, 50)) %>% 
  mutate(model = map(K, ~ stm(out$documents, 
                              out$vocab,
                              data=out$meta,
                              prevalence =  ~ s(date_num, degree = 2) +
                                dummy_female * dummy_white,
                              K = .,
                              verbose = FALSE)))
stm_20_30_40_50

#took about 20 min on my PC
```

```{r, eval = FALSE, include=FALSE}
save.image('C:/Users/camil/Desktop/CSS Session/ted_all_topic_models.RData')
```

```{r message=FALSE, include=FALSE}
load("C:/Users/camil/Desktop/CSS Session/ted_all_topic_models.RData")
```

```{r message=FALSE, warning=FALSE}
#extract objects for single k values to perform regression and other analysis
stm20<-stm_20_30_40_50[[1,2]]
stm30<-stm_20_30_40_50[[2,2]]
stm40<-stm_20_30_40_50[[3,2]]
stm50<-stm_20_30_40_50[[4,2]]
```

```{r message=FALSE, warning=FALSE}
stm50
```

### Validation and Evaluation based on statistical measures

There is a variety of statistical measures to evaluate your model, see for instance [here](https://juliasilge.com/blog/evaluating-stm/).

- **Semantic coherence**: how often do terms that have a high probability of belonging to one topic, also co-occur in the respective document?

- However, do not only consider semantic coherence (can easily be improved by simply modeling fewer topels) but also consider exclusivity
- **Exclusivity**: How exclusive are the terms that occur with high probability for a topic; put differently: are they for other topics very unlikely?

In my opinion most insights are generated by plotting the two measures against each other:

**Short Interpretation?** The higher the semantic coherence and the higher the exclusivity of words within a topic, the "better" a topic model.

```{r message=FALSE, warning=FALSE}
# calculate exclusivity + semantic coherence
model_scores <- stm_20_30_40_50 %>% 
  mutate(exclusivity = map(model, exclusivity),
         semantic_coherence = map(model, semanticCoherence, out$documents)) %>% 
  select(K, exclusivity, semantic_coherence)

model_scores #results in nested dataframes containing the quantities of interest

# plot
model_scores %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(20, 30, 50)) %>%
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence")

# upper right area shows "good" topic model specifications

# plot means for better overview
model_scores %>% 
  unnest(c(exclusivity, semantic_coherence)) %>% 
  group_by(K) %>% 
  summarize(exclusivity = mean(exclusivity),
            semantic_coherence = mean(semantic_coherence)) %>% 
  ggplot(aes(x = semantic_coherence, y = exclusivity, color = as.factor(K))) +
  geom_point() +
  theme_bw()
```

## $\rightarrow$ TED Talks contain 30 topics!

## Visualizations

After inspecting the different topic models, we found that the model with 30 topics includes the most useful topics for our analysis. 

Now let’s see what this topic model tells us about the Ted Talk Corpus and our research questions.

### Highest word probabilities for each topic

In order to gain valuable insights into the topics found and to be able to make nice visualizations, I would recommend using ggplot2. 

To do so, first, we have to transform our stm-object to a ggplot2- and tidyverse compatible format using the {tidytext}-package.

```{r message=FALSE, warning=FALSE}
td_beta <- tidy(stm30[[1]]) 
td_beta
```

One good idea is to plot the most likely words per topic. For this we have to extract the beta values from out topic model (beta is the probability of words belonging to topics) and perform some ggplot coding.

```{r message=FALSE, warning=FALSE}
td_beta %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "Different words are associated with different topics")
```

Unfortunately, with this high number of topics (remember, k is set to 30 here), the small multiples get quite small. Maybe consider to use a shiny app to explore all your topics. For an demonstration see [here](https://juliasilge.shinyapps.io/sherlock-holmes/#section-words-by-topic).

### One final plot 

... originally from [Julia Silge](https://juliasilge.com/blog/evaluating-stm/) `r emo::ji("+1")`

```{r message=FALSE, warning=FALSE}
td_gamma <- tidy(stm30[[1]], matrix = "gamma",
                 document_names = rownames(ted_dfm))

top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(7, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(20, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3,
            family = "IBMPlexSans") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.09),
                     labels = percent_format()) +
  theme_tufte(base_family = "IBMPlexSans", ticks = FALSE) +
  theme(plot.title = element_text(size = 16,
                                  family="IBMPlexSans-Bold"),
        plot.subtitle = element_text(size = 13)) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence in the TED Talks corpus",
       subtitle = "With the top words that contribute to each topic")
```

